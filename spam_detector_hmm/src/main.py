"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å–ø–∞–º–∞
"""
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from data_loader import DataLoader
from preprocessor import TextPreprocessor
from hmm_detector import SpamDetectorHMM
from visualizer import Visualizer

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def print_header(text):
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    print("\n" + "="*70)
    print(f"  {text}")
    print("="*70)

def analyze_text_detail(detector, preprocessor, text, true_label):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    
    Args:
        detector: –æ–±—É—á–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
        preprocessor: –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        true_label: –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞
    """
    print("\n" + "-"*70)
    print(f"üìÑ –¢–ï–ö–°–¢ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤):")
    print(f"   {text[:100]}...")
    print(f"   –ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {true_label.upper()}")
    print("-"*70)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    sequence = preprocessor.text_to_sequence(text)
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"   –î–ª–∏–Ω–∞: {len(sequence)}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(np.unique(sequence))}")
    print(f"   –ü–µ—Ä–≤—ã–µ 20 —Å–∏–º–≤–æ–ª–æ–≤: {sequence[:20]}")
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    result = detector.predict_proba(sequence)
    print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction'].upper()}")
    print(f"   Log P(X|Natural): {result['log_prob_natural']:.2f}")
    print(f"   Log P(X|Spam):    {result['log_prob_spam']:.2f}")
    print(f"   P(Natural|X):     {result['prob_natural']:.4f}")
    print(f"   P(Spam|X):        {result['prob_spam']:.4f}")
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –í–∏—Ç–µ—Ä–±–∏ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
    print(f"\nüîç –î–ï–ö–û–î–ò–†–û–í–ê–ù–ò–ï –í–ò–¢–ï–†–ë–ò:")
    
    for model_type in ['natural', 'spam']:
        viterbi_result = detector.decode_viterbi(sequence, model_type)
        states = viterbi_result['states']
        
        print(f"\n   –ú–æ–¥–µ–ª—å: {model_type.upper()}")
        print(f"   Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {viterbi_result['log_probability']:.2f}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–æ—Å—Ç–æ—è–Ω–∏–π: {viterbi_result['n_states_used']}/{detector.n_states}")
        print(f"   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–π (–ø–µ—Ä–≤—ã–µ 30): {states[:30]}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º
        unique, counts = np.unique(states, return_counts=True)
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º:")
        for state, count in zip(unique, counts):
            percentage = (count / len(states)) * 100
            print(f"      –°–æ—Å—Ç–æ—è–Ω–∏–µ {state}: {count} —Ä–∞–∑ ({percentage:.1f}%)")
    
    # –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    is_correct = (result['prediction'] == true_label)
    print(f"\n{'‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û' if is_correct else '‚ùå –û–®–ò–ë–ö–ê'}")
    print("-"*70)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print_header("–î–ï–¢–ï–ö–¢–û–† –ü–û–ò–°–ö–û–í–û–ì–û –°–ü–ê–ú–ê –ù–ê –û–°–ù–û–í–ï HMM")
    print("–°–∫—Ä—ã—Ç—ã–µ –º–∞—Ä–∫–æ–≤—Å–∫–∏–µ –º–æ–¥–µ–ª–∏: –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ë–∞—É–º–∞-–í–µ–ª—à–∞ –∏ –í–∏—Ç–µ—Ä–±–∏")
    
    # ==================== –®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ====================
    print_header("–®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    
    natural_texts, spam_texts = DataLoader.load_train_data()
    
    if len(natural_texts) < 3 or len(spam_texts) < 3:
        print("\n‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ sample_texts.json")
        sample_data = DataLoader.load_sample_data()
        natural_texts = sample_data['natural'] * 3  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –æ–±—ä—ë–º–∞
        spam_texts = sample_data['spam'] * 3
        test_texts_examples = sample_data['test']
    else:
        test_texts_examples = []
    
    print(f"\nüìö –ò—Ç–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(natural_texts)}")
    print(f"   –°–ø–∞–º-—Ç–µ–∫—Å—Ç–æ–≤: {len(spam_texts)}")
    
    # ==================== –®–ê–ì 2: –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• (–î–û –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò!) ====================
    print_header("–®–ê–ì 2: –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê –û–ë–£–ß–ê–Æ–©–£–Æ –ò –¢–ï–°–¢–û–í–£–Æ –í–´–ë–û–†–ö–ò")
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –†–∞–∑–¥–µ–ª—è–µ–º –ü–ï–†–ï–î –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º —Å–ª–æ–≤–∞—Ä—è!
    natural_train_texts, natural_test_texts = train_test_split(
        natural_texts, test_size=0.2, random_state=42
    )
    spam_train_texts, spam_test_texts = train_test_split(
        spam_texts, test_size=0.2, random_state=42
    )
    
    print(f"\nüì¶ –†–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –≤—ã–±–æ—Ä–æ–∫:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:")
    print(f"      –û–±—ã—á–Ω—ã–µ: {len(natural_train_texts)}")
    print(f"      –°–ø–∞–º: {len(spam_train_texts)}")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:")
    print(f"      –û–±—ã—á–Ω—ã–µ: {len(natural_test_texts)}")
    print(f"      –°–ø–∞–º: {len(spam_test_texts)}")
    
    # ==================== –®–ê–ì 3: –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê ====================
    print_header("–®–ê–ì 3: –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í")
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor = TextPreprocessor(feature_type='pos', n_symbols=50)
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –¢–û–õ–¨–ö–û –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö!
    train_texts = natural_train_texts + spam_train_texts
    print(f"\nüìñ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ {len(train_texts)} –æ–±—É—á–∞—é—â–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö...")
    preprocessor.build_vocabulary(train_texts)
    
    print(f"\nüî§ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {preprocessor.get_vocabulary_size()}")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\nüîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    
    natural_train = preprocessor.texts_to_sequences(natural_train_texts)
    spam_train = preprocessor.texts_to_sequences(spam_train_texts)
    natural_test = preprocessor.texts_to_sequences(natural_test_texts)
    spam_test = preprocessor.texts_to_sequences(spam_test_texts)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º
    natural_train_lengths = [len(seq) for seq in natural_train]
    spam_train_lengths = [len(seq) for seq in spam_train]
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
    print(f"   –û–±—É—á–∞—é—â–∞—è - –û–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã:")
    print(f"      –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {np.mean(natural_train_lengths):.1f}")
    print(f"      –ú–∏–Ω/–ú–∞–∫—Å: {np.min(natural_train_lengths)}/{np.max(natural_train_lengths)}")
    print(f"   –û–±—É—á–∞—é—â–∞—è - –°–ø–∞–º-—Ç–µ–∫—Å—Ç—ã:")
    print(f"      –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {np.mean(spam_train_lengths):.1f}")
    print(f"      –ú–∏–Ω/–ú–∞–∫—Å: {np.min(spam_train_lengths)}/{np.max(spam_train_lengths)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤:")
    vocab_size = preprocessor.get_vocabulary_size()
    
    all_train = natural_train + spam_train
    all_test = natural_test + spam_test
    
    max_train_idx = max([seq.max() for seq in all_train if len(seq) > 0])
    max_test_idx = max([seq.max() for seq in all_test if len(seq) > 0])
    
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    print(f"   Max –∏–Ω–¥–µ–∫—Å –≤ Train: {max_train_idx}")
    print(f"   Max –∏–Ω–¥–µ–∫—Å –≤ Test: {max_test_idx}")
    
    if max_train_idx >= vocab_size:
        print(f"   ‚ö†Ô∏è  –û–®–ò–ë–ö–ê: Train –∏–Ω–¥–µ–∫—Å—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã!")
    if max_test_idx >= vocab_size:
        print(f"   ‚ö†Ô∏è  –û–®–ò–ë–ö–ê: Test –∏–Ω–¥–µ–∫—Å—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã!")
    
    if max_train_idx < vocab_size and max_test_idx < vocab_size:
        print(f"   ‚úÖ –í—Å–µ –∏–Ω–¥–µ–∫—Å—ã –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –≥—Ä–∞–Ω–∏—Ü–∞—Ö")
    
    # ==================== –®–ê–ì 4: –û–ë–£–ß–ï–ù–ò–ï HMM ====================
    print_header("–®–ê–ì 4: –û–ë–£–ß–ï–ù–ò–ï –°–ö–†–´–¢–´–• –ú–ê–†–ö–û–í–°–ö–ò–• –ú–û–î–ï–õ–ï–ô")
    print("\n–ê–ª–≥–æ—Ä–∏—Ç–º: –ë–∞—É–º–∞-–í–µ–ª—à–∞ (EM-–∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è HMM)")
    
    # –°–æ–∑–¥–∞—ë–º –¥–µ—Ç–µ–∫—Ç–æ—Ä
    detector = SpamDetectorHMM(n_states=3, n_iter=100, tol=1e-2)
    
    # –û–±—É—á–∞–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    detector.fit(
        natural_sequences=natural_train,
        spam_sequences=spam_train,
        n_features=vocab_size
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
    detector.save('models/')
    
    # ==================== –®–ê–ì 5: –ê–ù–ê–õ–ò–ó –ú–û–î–ï–õ–ï–ô ====================
    print_header("–®–ê–ì 5: –ê–ù–ê–õ–ò–ó –û–ë–£–ß–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô")
    
    print("\nüìà –ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –¥–ª—è –û–ë–´–ß–ù–´–• —Ç–µ–∫—Å—Ç–æ–≤:")
    print(detector.get_transition_matrix('natural'))
    
    print("\nüìà –ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –¥–ª—è –°–ü–ê–ú —Ç–µ–∫—Å—Ç–æ–≤:")
    print(detector.get_transition_matrix('spam'))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
    Visualizer.plot_transition_matrices(detector)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —ç–º–∏—Å—Å–∏–π
    print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —ç–º–∏—Å—Å–∏–π...")
    Visualizer.plot_emission_distributions(detector, top_n=min(10, vocab_size))
    
    # ==================== –®–ê–ì 6: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================
    print_header("–®–ê–ì 6: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –û–¢–õ–û–ñ–ï–ù–ù–û–ô –í–´–ë–û–†–ö–ï")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    test_sequences = natural_test + spam_test
    true_labels = ['natural'] * len(natural_test) + ['spam'] * len(spam_test)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print(f"\nüîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ {len(test_sequences)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    predictions = detector.predict(test_sequences)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, pos_label='spam', zero_division=0)
    recall = recall_score(true_labels, predictions, pos_label='spam', zero_division=0)
    f1 = f1_score(true_labels, predictions, pos_label='spam', zero_division=0)
    
    print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê:")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   F1-Score:  {f1:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Visualizer.plot_classification_comparison(predictions, true_labels)
    
    # ==================== –®–ê–ì 7: –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ò–ú–ï–†–û–í ====================
    print_header("–®–ê–ì 7: –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ò–ú–ï–†–û–í (–ê–õ–ì–û–†–ò–¢–ú –í–ò–¢–ï–†–ë–ò)")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    if len(natural_test_texts) > 0 and len(spam_test_texts) > 0:
        print("\nüî¨ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –í–∏—Ç–µ—Ä–±–∏:")
        
        analyze_text_detail(
            detector, preprocessor, 
            natural_test_texts[0], 
            'natural'
        )
        
        analyze_text_detail(
            detector, preprocessor,
            spam_test_texts[0],
            'spam'
        )
    
    # ==================== –®–ê–ì 8: –ü–†–ò–ú–ï–†–´ –ò–ó SAMPLE ====================
    if test_texts_examples:
        print_header("–®–ê–ì 8: –ê–ù–ê–õ–ò–ó –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–†–ò–ú–ï–†–û–í")
        
        for idx, text in enumerate(test_texts_examples, 1):
            print(f"\n{'='*70}")
            print(f"–ü–†–ò–ú–ï–† {idx}")
            print(f"{'='*70}")
            print(f"–¢–µ–∫—Å—Ç: {text}")
            
            sequence = preprocessor.text_to_sequence(text)
            result = detector.predict_proba(sequence)
            
            print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction'].upper()}")
            print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (Natural): {result['prob_natural']:.4f}")
            print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (Spam):    {result['prob_spam']:.4f}")
            
            # –í–∏—Ç–µ—Ä–±–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            viterbi_natural = detector.decode_viterbi(sequence, 'natural')
            viterbi_spam = detector.decode_viterbi(sequence, 'spam')
            
            print(f"\n   –í–∏—Ç–µ—Ä–±–∏ (Natural model): log-prob = {viterbi_natural['log_probability']:.2f}")
            print(f"   –í–∏—Ç–µ—Ä–±–∏ (Spam model):    log-prob = {viterbi_spam['log_probability']:.2f}")
    
    # ==================== –ó–ê–í–ï–†–®–ï–ù–ò–ï ====================
    print_header("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù")
    
    print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("   ‚Ä¢ models/natural_model.pkl - –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
    print("   ‚Ä¢ models/spam_model.pkl - –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ø–∞–º–∞")
    print("   ‚Ä¢ transition_matrices.png - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –ø–µ—Ä–µ—Ö–æ–¥–æ–≤")
    print("   ‚Ä¢ emission_distributions.png - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–∏—Å—Å–∏–π")
    print("   ‚Ä¢ confusion_matrix.png - –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
    
    print("\nüí° –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:")
    print("   1. –ê–ª–≥–æ—Ä–∏—Ç–º –ë–∞—É–º–∞-–í–µ–ª—à–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏–ª –¥–≤–µ HMM –º–æ–¥–µ–ª–∏")
    print("   2. –ú–æ–¥–µ–ª–∏ –Ω–∞—É—á–∏–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–∞–º–∞")
    print("   3. –ê–ª–≥–æ—Ä–∏—Ç–º –í–∏—Ç–µ—Ä–±–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
    print(f"   4. –¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {accuracy*100:.2f}%")
    
    return detector, preprocessor

if __name__ == "__main__":
    try:
        detector, preprocessor = main()
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        print("\n" + "="*70)
        print("üéÆ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú")
        print("="*70)
        print("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):\n")
        
        while True:
            user_input = input("\nüìù –í–∞—à —Ç–µ–∫—Å—Ç: ").strip()
            
            if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not user_input:
                continue
            
            # –ê–Ω–∞–ª–∏–∑ –≤–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            sequence = preprocessor.text_to_sequence(user_input)
            result = detector.predict_proba(sequence)
            
            print(f"\n{'='*70}")
            print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê:")
            print(f"{'='*70}")
            print(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {result['prediction'].upper()}")
            print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (Natural): {result['prob_natural']:.4f}")
            print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (Spam):    {result['prob_spam']:.4f}")
            print(f"Log-likelihood —Ä–∞–∑–Ω–∏—Ü–∞: {abs(result['log_prob_natural'] - result['log_prob_spam']):.2f}")
            
            # –ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            if result['prediction'] == 'spam':
                confidence = result['prob_spam']
                if confidence > 0.9:
                    print("\n‚ö†Ô∏è  –í–´–°–û–ö–ê–Ø –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ø–∞–º–∞!")
                elif confidence > 0.7:
                    print("\n‚ö†Ô∏è  –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ø–∞–º–∞")
                else:
                    print("\n‚ö†Ô∏è  –ù–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ø–∞–º–∞ (–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)")
            else:
                print("\n‚úÖ –¢–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º")
            
            print(f"{'='*70}")
    
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()