"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ø–∞–º–∞ + –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from data_loader import DataLoader
from preprocessor import TextPreprocessor
from hmm_detector import SpamDetectorHMM
from visualizer import Visualizer
from markov_spam_generator import create_markov_spam_dataset

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from collections import Counter
import joblib

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
LAST_DETECTOR = None
LAST_PREPROCESSOR = None
LAST_SPAM_TYPE = None

def print_header(text):
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    print("\n" + "="*80)
    print(f"  {text}")
    print("="*80)

def analyze_text_interactive(detector, preprocessor, text):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ main.py)"""
    import numpy as np
    from collections import Counter

    print("\n" + "-"*80)
    print("üìÑ –ê–ù–ê–õ–ò–ó –í–ê–®–ï–ì–û –¢–ï–ö–°–¢–ê")
    print("-"*80)
    print(f"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤):")
    print(f"   {text[:150]}{'...' if len(text) > 150 else ''}")
    print("-"*80)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    tokens = preprocessor.tokenize(text)
    pos_tags = preprocessor.extract_pos_features(text)
    seq = preprocessor.text_to_sequence(text)
    unk_idx = preprocessor.vocab.get('UNK', preprocessor.get_vocabulary_size() - 1)
    unk_ratio = float((seq == unk_idx).sum()) / len(seq) if len(seq) > 0 else 0.0

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–ö–°–¢–ê:")
    print(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –¢–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(tokens)}")
    print(f"   POS-—Ç–µ–≥–æ–≤: {len(pos_tags)}")
    print(f"   –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {len(seq)}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(np.unique(seq))}")
    print(f"   –î–æ–ª—è UNK-—Ç–æ–∫–µ–Ω–æ–≤: {unk_ratio:.2%}")
    
    if len(pos_tags) > 0:
        common_pos = Counter(pos_tags).most_common(5)
        print(f"   –¢–æ–ø-5 POS —Ç–µ–≥–æ–≤: {common_pos}")

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    result = detector.predict_proba(seq)
    
    print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
    print(f"   {'='*70}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction'].upper()}")
    print(f"   {'='*70}")
    print(f"   Log P(X|Natural): {result['log_prob_natural']:.2f}")
    print(f"   Log P(X|Spam):    {result['log_prob_spam']:.2f}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞:          {abs(result['log_prob_natural'] - result['log_prob_spam']):.2f}")
    print(f"   {'='*70}")
    print(f"   P(Natural|X):     {result['prob_natural']:.4f} ({result['prob_natural']*100:.2f}%)")
    print(f"   P(Spam|X):        {result['prob_spam']:.4f} ({result['prob_spam']*100:.2f}%)")
    print(f"   {'='*70}")

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if result['prediction'] == 'spam':
        confidence = result['prob_spam']
        if confidence > 0.9:
            print("\n‚ö†Ô∏è  üî¥ –í–´–°–û–ö–ê–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ –°–ü–ê–ú–ê!")
        elif confidence > 0.7:
            print("\n‚ö†Ô∏è  üü† –°–†–ï–î–ù–Ø–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ –°–ü–ê–ú–ê")
        else:
            print("\n‚ö†Ô∏è  üü° –ù–ò–ó–ö–ê–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ –°–ü–ê–ú–ê (–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)")
    else:
        confidence = result['prob_natural']
        if confidence > 0.9:
            print("\n‚úÖ üü¢ –í–´–°–û–ö–ê–Ø –£–í–ï–†–ï–ù–ù–û–°–¢–¨: –û–ë–´–ß–ù–´–ô –¢–ï–ö–°–¢")
        elif confidence > 0.7:
            print("\n‚úÖ üü¢ –°–†–ï–î–ù–Ø–Ø –£–í–ï–†–ï–ù–ù–û–°–¢–¨: –û–ë–´–ß–ù–´–ô –¢–ï–ö–°–¢")
        else:
            print("\n‚úÖ üü° –ù–ò–ó–ö–ê–Ø –£–í–ï–†–ï–ù–ù–û–°–¢–¨ (–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)")

    # –í–∏—Ç–µ—Ä–±–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    print(f"\nüîç –ê–õ–ì–û–†–ò–¢–ú –í–ò–¢–ï–†–ë–ò (–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π):")
    
    for model_type in ['natural', 'spam']:
        vit = detector.decode_viterbi(seq, model_type=model_type)
        gamma = detector.get_posteriors(seq, model_type=model_type)
        
        print(f"\n   üìà {model_type.upper()} –º–æ–¥–µ–ª—å:")
        print(f"      –í–∏—Ç–µ—Ä–±–∏ log-prob: {vit['log_probability']:.2f}")
        print(f"      –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π: {vit['n_states_used']}/{detector.n_states}")
        
        if gamma.size > 0:
            avg_gamma = gamma.mean(axis=0)
            print(f"      –°—Ä–µ–¥–Ω–∏–µ –ø–æ—Å—Ç–µ—Ä–∏–æ—Ä—ã: {np.round(avg_gamma, 3)}")
        
        if len(vit['states']) > 0:
            print(f"      –ü—É—Ç—å (–ø–µ—Ä–≤—ã–µ 30): {vit['states'][:30]}")
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º
            unique, counts = np.unique(vit['states'], return_counts=True)
            print(f"      –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π:")
            for state, count in zip(unique, counts):
                percentage = (count / len(vit['states'])) * 100
                print(f"         –°–æ—Å—Ç–æ—è–Ω–∏–µ {state}: {count} —Ä–∞–∑ ({percentage:.1f}%)")

    print("-"*80)

def interactive_testing_mode(detector, preprocessor, spam_type):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
    print_header(f"–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï - {spam_type}")
    
    print("üí° –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ –∫–æ–º–∞–Ω–¥—É):")
    print("   ‚Ä¢ –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏ –Ω–∞–∂–º–∏—Ç–µ Enter")
    print("   ‚Ä¢ 'exit' –∏–ª–∏ 'quit' ‚Äî –≤—ã—Ö–æ–¥")
    print("   ‚Ä¢ 'example' ‚Äî —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã")
    print("   ‚Ä¢ 'stats' ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏")
    print("   ‚Ä¢ 'graphs' ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏")
    
    while True:
        print("\n" + "="*80)
        user_input = input("üìù –í–∞—à —Ç–µ–∫—Å—Ç: ").strip()
        
        if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥', 'q']:
            print("üëã –í—ã—Ö–æ–¥ –∏–∑ —Ä–µ–∂–∏–º–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            break
        
        if not user_input:
            print("‚ö†Ô∏è  –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")
            continue
        
        if user_input.lower() == 'example':
            test_examples(detector, preprocessor)
            continue
        
        if user_input.lower() == 'stats':
            show_model_stats(detector, preprocessor)
            continue
        
        if user_input.lower() == 'graphs':
            show_all_visualizations(detector, preprocessor)
            continue
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        try:
            analyze_text_interactive(detector, preprocessor, user_input)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()

def test_examples(detector, preprocessor):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö"""
    examples = [
        ("Machine learning is a method of data analysis that automates analytical model building.", "natural"),
        ("Buy cheap pills online pharmacy discount best price now click here!!!", "spam"),
        ("Climate change refers to long-term shifts in temperatures and weather patterns.", "natural"),
        ("Win money casino gambling bonus free spins jackpot limited time offer!", "spam"),
        ("Python is an interpreted high-level programming language with dynamic semantics.", "natural")
    ]
    
    print("\n" + "="*80)
    print("–¢–ï–°–¢–û–í–´–ï –ü–†–ò–ú–ï–†–´")
    print("="*80)
    
    for i, (text, expected) in enumerate(examples, 1):
        print(f"\n{'‚îÄ'*80}")
        print(f"–ü–†–ò–ú–ï–† {i} (–æ–∂–∏–¥–∞–µ—Ç—Å—è: {expected.upper()})")
        print(f"{'‚îÄ'*80}")
        analyze_text_interactive(detector, preprocessor, text)

def show_model_stats(detector, preprocessor):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("\n" + "="*80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–û–î–ï–õ–ò")
    print("="*80)
    
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã HMM:")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π: {detector.n_states}")
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {detector.n_features}")
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (preprocessor): {preprocessor.get_vocabulary_size()}")
    
    print(f"\nüî§ –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ:")
    vocab_items = list(preprocessor.vocab.items())[:10]
    for feature, idx in vocab_items:
        print(f"      {feature}: {idx}")
    
    print(f"\nüìà –ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (Natural):")
    print(detector.get_transition_matrix('natural'))
    
    print(f"\nüìà –ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (Spam):")
    print(detector.get_transition_matrix('spam'))

def show_all_visualizations(detector, preprocessor):
    """–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∫–∞–∫ –≤ main.py"""
    print_header("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô")
    
    try:
        print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
        
        # 1. –ú–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
        print("   ‚Ä¢ –ú–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤...")
        Visualizer.plot_transition_matrices(detector)
        
        # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–∏—Å—Å–∏–π
        print("   ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–∏—Å—Å–∏–π...")
        vocab_size = preprocessor.get_vocabulary_size()
        Visualizer.plot_emission_distributions(detector, top_n=min(10, vocab_size))
        
        # 3. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("   ‚Ä¢ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        Visualizer.plot_feature_importance(preprocessor, top_n=20)
        
        print("\n‚úÖ –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

def train_model(natural_texts, spam_texts, description):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    global LAST_DETECTOR, LAST_PREPROCESSOR, LAST_SPAM_TYPE
    
    print(f"\nüîß –û–ë–£–ß–ï–ù–ò–ï –ù–ê: {description}")
    print("-"*50)
    
    if len(natural_texts) < 2 or len(spam_texts) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 2 —Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞)")
        return None, None
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    natural_train_texts, natural_test_texts = train_test_split(
        natural_texts, test_size=0.2, random_state=42
    )
    spam_train_texts, spam_test_texts = train_test_split(
        spam_texts, test_size=0.2, random_state=42
    )
    
    print(f"üì¶ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   Train Natural: {len(natural_train_texts)}")
    print(f"   Test Natural:  {len(natural_test_texts)}")
    print(f"   Train Spam:    {len(spam_train_texts)}")
    print(f"   Test Spam:     {len(spam_test_texts)}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    preprocessor = TextPreprocessor(feature_type='pos', n_symbols=50)
    train_texts = natural_train_texts + spam_train_texts
    print(f"\nüîÑ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ {len(train_texts)} —Ç–µ–∫—Å—Ç–∞—Ö...")
    preprocessor.build_vocabulary(train_texts)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    natural_train = preprocessor.texts_to_sequences(natural_train_texts)
    spam_train = preprocessor.texts_to_sequences(spam_train_texts)
    natural_test = preprocessor.texts_to_sequences(natural_test_texts)
    spam_test = preprocessor.texts_to_sequences(spam_test_texts)
    
    # –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    print(f"\nüß† –û–±—É—á–µ–Ω–∏–µ HMM –º–æ–¥–µ–ª–µ–π...")
    detector = SpamDetectorHMM(n_states=3, n_iter=100)
    detector.fit(natural_train, spam_train, preprocessor.get_vocabulary_size())
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ...")
    test_sequences = natural_test + spam_test
    true_labels = ['natural'] * len(natural_test) + ['spam'] * len(spam_test)
    predictions = detector.predict(test_sequences)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, pos_label='spam', zero_division=0)
    recall = recall_score(true_labels, predictions, pos_label='spam', zero_division=0)
    f1 = f1_score(true_labels, predictions, pos_label='spam', zero_division=0)
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ ({description}):")
    print(f"   Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}%)")
    print(f"   Precision: {precision:.3f}")
    print(f"   Recall:    {recall:.3f}")
    print(f"   F1-Score:  {f1:.3f}")
    
    # Classification Report
    print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
    print(classification_report(true_labels, predictions, 
                               target_names=['Natural', 'Spam'], 
                               zero_division=0))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    Visualizer.plot_classification_comparison(predictions, true_labels)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    print(f"   ‚Ä¢ –ú–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤...")
    Visualizer.plot_transition_matrices(detector)
    
    print(f"   ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–∏—Å—Å–∏–π...")
    vocab_size = preprocessor.get_vocabulary_size()
    Visualizer.plot_emission_distributions(detector, top_n=min(10, vocab_size))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_path = f'models/{description.replace(" ", "_").lower()}'
    Path(model_path).mkdir(parents=True, exist_ok=True)
    detector.save(model_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    joblib.dump(preprocessor, f'{model_path}/preprocessor.pkl')
    print(f"‚úì –ú–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {model_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
    LAST_DETECTOR = detector
    LAST_PREPROCESSOR = preprocessor
    LAST_SPAM_TYPE = description
    
    return detector, preprocessor



def load_saved_model(spam_type):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    global LAST_DETECTOR, LAST_PREPROCESSOR, LAST_SPAM_TYPE
    
    model_name = spam_type.replace(" ", "_").lower()
    model_path = f'models/{model_name}'
    
    if not Path(model_path).exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return None, None
    
    try:
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
        
        detector = SpamDetectorHMM()
        detector.load(model_path)
        
        preprocessor = joblib.load(f'{model_path}/preprocessor.pkl')
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(f"   –¢–∏–ø: {spam_type}")
        print(f"   –°–æ—Å—Ç–æ—è–Ω–∏–π: {detector.n_states}")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {preprocessor.get_vocabulary_size()}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
        LAST_DETECTOR = detector
        LAST_PREPROCESSOR = preprocessor
        LAST_SPAM_TYPE = spam_type
        
        return detector, preprocessor
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def list_available_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    models_dir = Path('models')
    if not models_dir.exists():
        return []
    
    available_models = []
    for model_path in models_dir.iterdir():
        if model_path.is_dir():
            metadata_file = model_path / 'metadata.pkl'
            if metadata_file.exists():
                available_models.append(model_path.name)
    
    return available_models

def show_menu():
    """–ü–æ–∫–∞–∑–∞—Ç—å –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é"""
    print_header("–î–ï–¢–ï–ö–¢–û–† –°–ü–ê–ú–ê - –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø")
    
    if LAST_DETECTOR:
        print(f"üîÑ –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: {LAST_SPAM_TYPE}")
    else:
        print(f"‚ö™ –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    print("\nüéØ –†–ï–ñ–ò–ú–´ –†–ê–ë–û–¢–´:")
    print("\nüìö –û–ë–£–ß–ï–ù–ò–ï:")
    print("1. üß† –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —Å–ø–∞–º–µ")
    print("2. ü§ñ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Ä–∫–æ–≤—Å–∫–æ–º —Å–ø–∞–º–µ")
    print("3. üîÑ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
    print("4. üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤")
    
    print("\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:")
    print("5. üéÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å)")
    print("6. üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
    print("7. üß™ –¢–µ—Å—Ç –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö (unseen data)")  # ‚Üê –ù–û–í–û–ï!
    
    print("\nüõ†Ô∏è –£–¢–ò–õ–ò–¢–´:")
    print("8. ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–ø–∞–º–∞")
    print("9. üìà –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("10. üöÄ –ê–≤—Ç–æ–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("11. üìù –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç")  # ‚Üê –ù–û–í–û–ï!
    print("12. ‚ùå –í—ã—Ö–æ–¥")
    
    choice = input("\nüìã –í–∞—à –≤—ã–±–æ—Ä (1-12): ").strip()
    return choice


def train_on_human_spam():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —Å–ø–∞–º–µ"""
    print_header("–û–ë–£–ß–ï–ù–ò–ï –ù–ê –ß–ï–õ–û–í–ï–ß–ï–°–ö–û–ú –°–ü–ê–ú–ï")
    
    datasets = DataLoader.load_all_data()
    
    if not datasets['human_spam']:
        print("‚ùå –ù–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Å–ø–∞–º–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏—è 9)")
        return None, None
    
    print(f"\nüìä –î–∞–Ω–Ω—ã–µ:")
    print(f"   Natural —Ç–µ–∫—Å—Ç—ã: {len(datasets['human_natural'])}")
    print(f"   Human spam: {len(datasets['human_spam'])}")
    
    result = train_model(datasets['human_natural'], datasets['human_spam'], "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Å–ø–∞–º")
    
    if result[0]:
        choice = input("\nüí° –•–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ? (y/n): ").strip().lower()
        if choice in ['y', 'yes', '–¥', '–¥–∞']:
            interactive_testing_mode(result[0], result[1], "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Å–ø–∞–º")
    
    return result

def train_on_markov_spam():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Ä–∫–æ–≤—Å–∫–æ–º —Å–ø–∞–º–µ"""
    print_header("–û–ë–£–ß–ï–ù–ò–ï –ù–ê –ú–ê–†–ö–û–í–°–ö–û–ú –°–ü–ê–ú–ï")
    
    datasets = DataLoader.load_all_data()
    
    if not datasets['markov_spam']:
        print("‚ö†Ô∏è –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω! –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º...")
        create_markov_spam_dataset(50)
        datasets = DataLoader.load_all_data()
    
    if not datasets['markov_spam']:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º!")
        return None, None
    
    print(f"\nüìä –î–∞–Ω–Ω—ã–µ:")
    print(f"   Natural —Ç–µ–∫—Å—Ç—ã: {len(datasets['human_natural'])}")
    print(f"   Markov spam: {len(datasets['markov_spam'])}")
    
    result = train_model(datasets['human_natural'], datasets['markov_spam'], "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º")
    
    if result[0]:
        choice = input("\nüí° –•–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ? (y/n): ").strip().lower()
        if choice in ['y', 'yes', '–¥', '–¥–∞']:
            interactive_testing_mode(result[0], result[1], "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º")
    
    return result

def train_on_mixed_dataset():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print_header("–û–ë–£–ß–ï–ù–ò–ï –ù–ê –°–ú–ï–®–ê–ù–ù–û–ú –î–ê–¢–ê–°–ï–¢–ï")
    
    datasets = DataLoader.load_all_data()
    
    if not datasets['markov_spam']:
        print("‚ö†Ô∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º...")
        create_markov_spam_dataset(50)
        datasets = DataLoader.load_all_data()
    
    mixed_spam = datasets['human_spam'] + datasets.get('markov_spam', [])
    
    if not mixed_spam:
        print("‚ùå –ù–µ—Ç —Å–ø–∞–º-–¥–∞–Ω–Ω—ã—Ö!")
        return None, None
    
    print(f"\nüìä –°–º–µ—à–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç:")
    print(f"   Natural —Ç–µ–∫—Å—Ç—ã: {len(datasets['human_natural'])}")
    print(f"   Human spam: {len(datasets['human_spam'])}")
    print(f"   Markov spam: {len(datasets.get('markov_spam', []))}")
    print(f"   –û–±—â–∏–π —Å–ø–∞–º: {len(mixed_spam)}")
    
    result = train_model(datasets['human_natural'], mixed_spam, "–°–º–µ—à–∞–Ω–Ω—ã–π —Å–ø–∞–º")
    
    if result[0]:
        choice = input("\nüí° –•–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ? (y/n): ").strip().lower()
        if choice in ['y', 'yes', '–¥', '–¥–∞']:
            interactive_testing_mode(result[0], result[1], "–°–º–µ—à–∞–Ω–Ω—ã–π —Å–ø–∞–º")
    
    return result

def compare_spam_types():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ø–∞–º–∞"""
    print_header("–°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –°–ü–ê–ú–ê")
    
    datasets = DataLoader.load_all_data()
    results = {}
    
    if datasets['human_spam']:
        detector1, _ = train_model(datasets['human_natural'], datasets['human_spam'], "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Å–ø–∞–º")
        if detector1:
            results['human_only'] = detector1
    
    if datasets['markov_spam'] or datasets['human_spam']:
        if not datasets['markov_spam']:
            create_markov_spam_dataset(50)
            datasets = DataLoader.load_all_data()
        
        if datasets['markov_spam']:
            detector2, _ = train_model(datasets['human_natural'], datasets['markov_spam'], "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º")
            if detector2:
                results['markov_only'] = detector2
    
    if datasets['human_spam'] and datasets['markov_spam']:
        mixed = datasets['human_spam'] + datasets['markov_spam']
        detector3, _ = train_model(datasets['human_natural'], mixed, "–°–º–µ—à–∞–Ω–Ω—ã–π —Å–ø–∞–º")
        if detector3:
            results['mixed'] = detector3
    
    if results:
        print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        try:
            Visualizer.plot_comparison(results)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    
    return results

def test_on_external_data(detector, preprocessor, spam_type):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–Ω–µ—à–Ω–µ–º (unseen) –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print_header(f"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –í–ù–ï–®–ù–ò–• –î–ê–ù–ù–´–• - {spam_type}")
    
    test_dir = Path('data/test')
    
    if not test_dir.exists():
        print("‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python create_test_dataset.py")
        
        choice = input("\n–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–µ–π—á–∞—Å? (y/n): ").strip().lower()
        if choice in ['y', 'yes', '–¥', '–¥–∞']:
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from spam_detector_hmm.src.create_test_dataset import save_test_dataset
            save_test_dataset()
        else:
            return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_natural_dir = test_dir / 'natural'
    test_spam_dir = test_dir / 'spam'
    
    test_natural_texts = []
    for file_path in test_natural_dir.glob('*.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            test_natural_texts.append(f.read())
    
    test_spam_texts = []
    for file_path in test_spam_dir.glob('*.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            test_spam_texts.append(f.read())
    
    print(f"\nüìä –í–Ω–µ—à–Ω–∏–π —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç:")
    print(f"   Natural —Ç–µ–∫—Å—Ç—ã: {len(test_natural_texts)}")
    print(f"   Spam —Ç–µ–∫—Å—Ç—ã: {len(test_spam_texts)}")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    natural_test_seqs = preprocessor.texts_to_sequences(test_natural_texts)
    spam_test_seqs = preprocessor.texts_to_sequences(test_spam_texts)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    test_sequences = natural_test_seqs + spam_test_seqs
    true_labels = ['natural'] * len(natural_test_seqs) + ['spam'] * len(spam_test_seqs)
    
    print(f"\nüîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
    predictions = detector.predict(test_sequences)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    errors = []
    for i, (true, pred, text) in enumerate(zip(true_labels, predictions, test_natural_texts + test_spam_texts)):
        if true != pred:
            errors.append({
                'index': i,
                'true': true,
                'predicted': pred,
                'text': text[:100]
            })
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
    
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, pos_label='spam', zero_division=0)
    recall = recall_score(true_labels, predictions, pos_label='spam', zero_division=0)
    f1 = f1_score(true_labels, predictions, pos_label='spam', zero_division=0)
    
    print(f"\n{'='*80}")
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –í–ù–ï–®–ù–ò–• –î–ê–ù–ù–´–• ({spam_type})")
    print(f"{'='*80}")
    print(f"   Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}%)")
    print(f"   Precision: {precision:.3f}")
    print(f"   Recall:    {recall:.3f}")
    print(f"   F1-Score:  {f1:.3f}")
    print(f"{'='*80}")
    
    # Confusion Matrix
    cm = confusion_matrix(true_labels, predictions, labels=['natural', 'spam'])
    print(f"\nüìã Confusion Matrix:")
    print(f"                 Predicted")
    print(f"              Natural  Spam")
    print(f"True Natural:    {cm[0][0]:3d}     {cm[0][1]:3d}")
    print(f"     Spam:       {cm[1][0]:3d}     {cm[1][1]:3d}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
    print(classification_report(true_labels, predictions, 
                               target_names=['Natural', 'Spam'], 
                               zero_division=0))
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    if errors:
        print(f"\n‚ùå –û–®–ò–ë–ö–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ({len(errors)}):")
        for error in errors[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"\n   –ü—Ä–∏–º–µ—Ä #{error['index']}:")
            print(f"      –ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {error['true']}")
            print(f"      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {error['predicted']}")
            print(f"      –¢–µ–∫—Å—Ç: {error['text']}...")
    else:
        print(f"\n‚úÖ –ù–ï–¢ –û–®–ò–ë–û–ö! –ò–¥–µ–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è!")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    Visualizer.plot_classification_comparison(predictions, true_labels)
    
    return accuracy, precision, recall, f1


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ–ª–Ω—ã–º –º–µ–Ω—é"""
    global LAST_DETECTOR, LAST_PREPROCESSOR, LAST_SPAM_TYPE
    
    while True:
        choice = show_menu()
        
        try:
            # ==================== –û–ë–£–ß–ï–ù–ò–ï ====================
            if choice == '1':
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —Å–ø–∞–º–µ
                try:
                    result = train_on_human_spam()
                    if result and result[0]:
                        print("\n" + "="*80)
                        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
                        print("="*80)
                        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: models/—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π_—Å–ø–∞–º/")
                        print(f"üìä –¢–∏–ø —Å–ø–∞–º–∞: –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π")
                        print(f"üî¢ –°–æ—Å—Ç–æ—è–Ω–∏–π HMM: {result[0].n_states}")
                        print(f"üìñ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {result[1].get_vocabulary_size()}")
                    else:
                        print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
                        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–ø—Ü–∏—é 10 –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
                    import traceback
                    traceback.print_exc()
                
            elif choice == '2':
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Ä–∫–æ–≤—Å–∫–æ–º —Å–ø–∞–º–µ
                try:
                    result = train_on_markov_spam()
                    if result and result[0]:
                        print("\n" + "="*80)
                        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
                        print("="*80)
                        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: models/–º–∞—Ä–∫–æ–≤—Å–∫–∏–π_—Å–ø–∞–º/")
                        print(f"üìä –¢–∏–ø —Å–ø–∞–º–∞: –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π (—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)")
                        print(f"üî¢ –°–æ—Å—Ç–æ—è–Ω–∏–π HMM: {result[0].n_states}")
                        print(f"üìñ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {result[1].get_vocabulary_size()}")
                    else:
                        print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
                    import traceback
                    traceback.print_exc()
                
            elif choice == '3':
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
                try:
                    result = train_on_mixed_dataset()
                    if result and result[0]:
                        print("\n" + "="*80)
                        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
                        print("="*80)
                        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: models/—Å–º–µ—à–∞–Ω–Ω—ã–π_—Å–ø–∞–º/")
                        print(f"üìä –¢–∏–ø —Å–ø–∞–º–∞: –°–º–µ—à–∞–Ω–Ω—ã–π (—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π + –º–∞—Ä–∫–æ–≤—Å–∫–∏–π)")
                        print(f"üî¢ –°–æ—Å—Ç–æ—è–Ω–∏–π HMM: {result[0].n_states}")
                        print(f"üìñ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {result[1].get_vocabulary_size()}")
                    else:
                        print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
                    import traceback
                    traceback.print_exc()
                
            elif choice == '4':
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Å–ø–∞–º–∞
                try:
                    print("‚è≥ –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
                    print("   –≠—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è (–æ–±—É—á–µ–Ω–∏–µ 3 –º–æ–¥–µ–ª–µ–π)...")
                    print("   –ü–æ–¥–æ–∂–¥–∏—Ç–µ...\n")
                    
                    results = compare_spam_types()
                    
                    if results:
                        print("\n" + "="*80)
                        print("‚úÖ –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
                        print("="*80)
                        print(f"üìä –û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(results)}")
                        print(f"üìà –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: model_comparison.png")
                        
                        for name in results.keys():
                            print(f"   ‚Ä¢ {name}")
                    else:
                        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")
                        print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏—è 9)")
                        
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================
            elif choice == '5':
                # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
                if LAST_DETECTOR and LAST_PREPROCESSOR:
                    try:
                        print(f"\nüéÆ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–ª—è: {LAST_SPAM_TYPE}")
                        interactive_testing_mode(LAST_DETECTOR, LAST_PREPROCESSOR, LAST_SPAM_TYPE)
                    except KeyboardInterrupt:
                        print("\n‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    except Exception as e:
                        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                        import traceback
                        traceback.print_exc()
                else:
                    print("‚ùå –ù–ï–¢ –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò!")
                    print("="*80)
                    print("üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:")
                    print("   1Ô∏è‚É£ –û–±—É—á–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏–∏ 1-3)")
                    print("   2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏—è 6)")
                    print("="*80)
                
            elif choice == '6':
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                print_header("–ó–ê–ì–†–£–ó–ö–ê –°–û–•–†–ê–ù–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
                
                try:
                    available = list_available_models()
                    
                    if not available:
                        print("‚ùå –ù–ï–¢ –°–û–•–†–ê–ù–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô!")
                        print("="*80)
                        print("üí° –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å:")
                        print("   ‚Ä¢ –û–ø—Ü–∏—è 1: –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Å–ø–∞–º")
                        print("   ‚Ä¢ –û–ø—Ü–∏—è 2: –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π —Å–ø–∞–º")
                        print("   ‚Ä¢ –û–ø—Ü–∏—è 3: –°–º–µ—à–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
                        print("="*80)
                    else:
                        print("\nüìÇ –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò:")
                        print(f"{'‚îÄ'*80}")
                        for i, model_name in enumerate(available, 1):
                            # –ö—Ä–∞—Å–∏–≤–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–Ω–∏
                            display_name = model_name.replace("_", " ").title()
                            print(f"   {i}. üì¶ {display_name}")
                        print(f"{'‚îÄ'*80}")
                        
                        model_choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏ (–∏–ª–∏ Enter –¥–ª—è –æ—Ç–º–µ–Ω—ã): ").strip()
                        
                        if model_choice.isdigit() and 1 <= int(model_choice) <= len(available):
                            model_name = available[int(model_choice) - 1]
                            spam_type = model_name.replace("_", " ").title()
                            
                            print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {spam_type}...")
                            result = load_saved_model(spam_type)
                            
                            if result[0]:
                                print("\n" + "="*80)
                                print("‚úÖ –ú–û–î–ï–õ–¨ –ó–ê–ì–†–£–ñ–ï–ù–ê –£–°–ü–ï–®–ù–û!")
                                print("="*80)
                                print(f"üì¶ –¢–∏–ø: {spam_type}")
                                print(f"üî¢ –°–æ—Å—Ç–æ—è–Ω–∏–π: {result[0].n_states}")
                                print(f"üìñ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {result[1].get_vocabulary_size()}")
                                print("="*80)
                                
                                choice = input("\nüí° –ù–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ? (y/n): ").strip().lower()
                                if choice in ['y', 'yes', '–¥', '–¥–∞']:
                                    interactive_testing_mode(result[0], result[1], spam_type)
                        elif model_choice:
                            print("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏")
                        else:
                            print("‚Ü©Ô∏è –û—Ç–º–µ–Ω–µ–Ω–æ")
                        
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                    import traceback
                    traceback.print_exc()
            
            elif choice == '7':
                # –¢–µ—Å—Ç –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö (unseen data)
                if LAST_DETECTOR and LAST_PREPROCESSOR:
                    try:
                        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {LAST_SPAM_TYPE}")
                        print("   –ù–∞ –≤–Ω–µ—à–Ω–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (unseen data)...")
                        test_on_external_data(LAST_DETECTOR, LAST_PREPROCESSOR, LAST_SPAM_TYPE)
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                        import traceback
                        traceback.print_exc()
                else:
                    print("‚ùå –ù–ï–¢ –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò!")
                    print("="*80)
                    print("üí° –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å:")
                    print("   ‚Ä¢ –û–ø—Ü–∏–∏ 1-3: –û–±—É—á–µ–Ω–∏–µ")
                    print("   ‚Ä¢ –û–ø—Ü–∏—è 6: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π")
                    print("="*80)
            
            # ==================== –£–¢–ò–õ–ò–¢–´ ====================
            elif choice == '8':
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–ø–∞–º–∞
                print_header("–ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–ê–†–ö–û–í–°–ö–û–ì–û –°–ü–ê–ú–ê")
                
                try:
                    count = input("–°–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å? (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100): ").strip()
                    
                    if count:
                        if not count.isdigit():
                            print("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥! –ò—Å–ø–æ–ª—å–∑—É–µ–º 100 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                            n = 100
                        else:
                            n = int(count)
                            if n <= 0:
                                print("‚ö†Ô∏è –ß–∏—Å–ª–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ 0! –ò—Å–ø–æ–ª—å–∑—É–µ–º 100")
                                n = 100
                            elif n > 1000:
                                print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ! –ú–∞–∫—Å–∏–º—É–º 1000 –∑–∞ —Ä–∞–∑")
                                n = 1000
                    else:
                        n = 100
                    
                    print(f"\n‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {n} –º–∞—Ä–∫–æ–≤—Å–∫–∏—Ö —Å–ø–∞–º-—Ç–µ–∫—Å—Ç–æ–≤...")
                    print("   –ü–æ–¥–æ–∂–¥–∏—Ç–µ...\n")
                    
                    create_markov_spam_dataset(n)
                    
                    print("\n" + "="*80)
                    print(f"‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
                    print("="*80)
                    print(f"üìä –°–æ–∑–¥–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {n}")
                    print(f"üìÇ –ü–∞–ø–∫–∞: data/raw/markov_spam/")
                    print("="*80)
                    
                except ValueError:
                    print("‚ùå –û—à–∏–±–∫–∞! –ò—Å–ø–æ–ª—å–∑—É–µ–º 100 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                    create_markov_spam_dataset(100)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                    import traceback
                    traceback.print_exc()
            
            elif choice == '9':
                # –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                print_header("–ê–ù–ê–õ–ò–ó –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –î–ê–ù–ù–´–•")
                
                try:
                    datasets = DataLoader.get_available_datasets()
                    
                    print("\nüìä –û–ë–£–ß–ê–Æ–©–ò–ï –î–ê–¢–ê–°–ï–¢–´:")
                    print(f"{'‚îÄ'*80}")
                    print(f"   üìÇ data/raw/natural/      ‚Üí {datasets.get('human_natural', 0):4d} —Ñ–∞–π–ª–æ–≤")
                    print(f"   üìÇ data/raw/spam/         ‚Üí {datasets.get('human_spam', 0):4d} —Ñ–∞–π–ª–æ–≤")
                    print(f"   üìÇ data/raw/markov_spam/  ‚Üí {datasets.get('markov_spam', 0):4d} —Ñ–∞–π–ª–æ–≤")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
                    test_natural_count = 0
                    test_spam_count = 0
                    test_dir = Path('data/test')
                    
                    if test_dir.exists():
                        if (test_dir / 'natural').exists():
                            test_natural_count = len(list((test_dir / 'natural').glob('*.txt')))
                        if (test_dir / 'spam').exists():
                            test_spam_count = len(list((test_dir / 'spam').glob('*.txt')))
                    
                    print(f"{'‚îÄ'*80}")
                    print("\nüß™ –¢–ï–°–¢–û–í–´–ï –î–ê–¢–ê–°–ï–¢–´:")
                    print(f"{'‚îÄ'*80}")
                    if test_natural_count > 0 or test_spam_count > 0:
                        print(f"   üìÇ data/test/natural/     ‚Üí {test_natural_count:4d} —Ñ–∞–π–ª–æ–≤")
                        print(f"   üìÇ data/test/spam/        ‚Üí {test_spam_count:4d} —Ñ–∞–π–ª–æ–≤")
                    else:
                        print("   ‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω")
                        print("   üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–ø—Ü–∏—é 11 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è")
                    
                    print(f"{'‚îÄ'*80}")
                    
                    # –ò—Ç–æ–≥–∏
                    total_train = sum(datasets.values())
                    total_test = test_natural_count + test_spam_count
                    total_all = total_train + total_test
                    
                    print("\nüìà –ò–¢–û–ì–û:")
                    print(f"{'‚îÄ'*80}")
                    print(f"   üéì –û–±—É—á–∞—é—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤:  {total_train:4d}")
                    print(f"   üß™ –¢–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤:   {total_test:4d}")
                    print(f"   üìä –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤:      {total_all:4d}")
                    print(f"{'‚îÄ'*80}")
                    
                    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                    print(f"{'‚îÄ'*80}")
                    
                    recommendations = []
                    
                    if total_train < 50:
                        recommendations.append("   ‚ö†Ô∏è  –ú–∞–ª–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 50)")
                        recommendations.append("       üëâ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–ø—Ü–∏—é 10 (–ê–≤—Ç–æ–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞)")
                    else:
                        recommendations.append("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
                    
                    if total_test < 20:
                        recommendations.append("   ‚ö†Ô∏è  –ú–∞–ª–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 20)")
                        recommendations.append("       üëâ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–ø—Ü–∏—é 11 (–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç)")
                    else:
                        recommendations.append("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                    
                    if datasets.get('markov_spam', 0) == 0:
                        recommendations.append("   üí° –ù–µ—Ç –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–ø–∞–º–∞")
                        recommendations.append("       üëâ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–ø—Ü–∏—é 8 (–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–ø–∞–º–∞)")
                    
                    for rec in recommendations:
                        print(rec)
                    
                    print(f"{'‚îÄ'*80}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    if total_train > 0:
                        print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...")
                        try:
                            Visualizer.plot_spam_type_distribution()
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫: {e}")
                
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
                    import traceback
                    traceback.print_exc()
            
            elif choice == '10':
                # –ê–≤—Ç–æ–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                print_header("–ê–í–¢–û–ü–û–î–ì–û–¢–û–í–ö–ê –í–°–ï–• –î–ê–ù–ù–´–•")
                
                try:
                    print("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–£–Æ –ü–û–î–ì–û–¢–û–í–ö–£ –î–ê–¢–ê–°–ï–¢–û–í")
                    print("="*80)
                    print("   –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç:")
                    print("   1. üìö –ó–∞–≥—Ä—É–∑–∫—É —Å—Ç–∞—Ç–µ–π –∏–∑ Wikipedia")
                    print("   2. üìñ –ó–∞–≥—Ä—É–∑–∫—É –∫–Ω–∏–≥ –∏–∑ Project Gutenberg")
                    print("   3. üìß –ó–∞–≥—Ä—É–∑–∫—É —Å–ø–∞–º-—Å–æ–æ–±—â–µ–Ω–∏–π")
                    print("   4. ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–ø–∞–º–∞")
                    print("\n‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
                    print("="*80)
                    
                    confirm = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/n): ").strip().lower()
                    
                    if confirm not in ['y', 'yes', '–¥', '–¥–∞']:
                        print("‚Ü©Ô∏è –û—Ç–º–µ–Ω–µ–Ω–æ")
                        continue
                    
                    print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞...\n")
                    
                    sys.path.insert(0, str(Path(__file__).parent.parent))
                    from prepare_datasets import prepare_all_datasets
                    
                    datasets = prepare_all_datasets()
                    
                    print("\n" + "="*80)
                    print("‚úÖ –ê–í–¢–û–ü–û–î–ì–û–¢–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
                    print("="*80)
                    print("üìä –ì–æ—Ç–æ–≤–æ –∫ –æ–±—É—á–µ–Ω–∏—é:")
                    for name, count in datasets.items():
                        print(f"   ‚Ä¢ {name}: {count} —Ç–µ–∫—Å—Ç–æ–≤")
                    print("="*80)
                    print("üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ (–æ–ø—Ü–∏–∏ 1-3)")
                    
                except ImportError as e:
                    print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –º–æ–¥—É–ª—å prepare_datasets: {e}")
                    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª prepare_datasets.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
                    import traceback
                    traceback.print_exc()
            
            elif choice == '11':
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                print_header("–°–û–ó–î–ê–ù–ò–ï –¢–ï–°–¢–û–í–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
                
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
                    test_dir = Path('data/test')
                    existing_natural = 0
                    existing_spam = 0
                    
                    if test_dir.exists():
                        if (test_dir / 'natural').exists():
                            existing_natural = len(list((test_dir / 'natural').glob('*.txt')))
                        if (test_dir / 'spam').exists():
                            existing_spam = len(list((test_dir / 'spam').glob('*.txt')))
                    
                    if existing_natural > 0 or existing_spam > 0:
                        print(f"‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç:")
                        print(f"   Natural: {existing_natural} —Ñ–∞–π–ª–æ–≤")
                        print(f"   Spam: {existing_spam} —Ñ–∞–π–ª–æ–≤")
                        
                        overwrite = input("\n–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å? (y/n): ").strip().lower()
                        if overwrite not in ['y', 'yes', '–¥', '–¥–∞']:
                            print("‚Ü©Ô∏è –û—Ç–º–µ–Ω–µ–Ω–æ")
                            continue
                    
                    print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
                    print("   ‚Ä¢ 20 –æ–±—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (natural)")
                    print("   ‚Ä¢ 20 —Å–ø–∞–º-—Ç–µ–∫—Å—Ç–æ–≤ (spam)")
                    print("\n‚è≥ –ü–æ–¥–æ–∂–¥–∏—Ç–µ...\n")
                    
                    sys.path.insert(0, str(Path(__file__).parent.parent))
                    from spam_detector_hmm.src.create_test_dataset import save_test_dataset
                    
                    nat_count, spam_count = save_test_dataset()
                    
                    print("\n" + "="*80)
                    print("‚úÖ –¢–ï–°–¢–û–í–´–ô –î–ê–¢–ê–°–ï–¢ –°–û–ó–î–ê–ù!")
                    print("="*80)
                    print(f"üìÇ –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ: data/test/")
                    print(f"   ‚Ä¢ data/test/natural/ ‚Üí {nat_count} —Ñ–∞–π–ª–æ–≤")
                    print(f"   ‚Ä¢ data/test/spam/    ‚Üí {spam_count} —Ñ–∞–π–ª–æ–≤")
                    print(f"   üìä –í—Å–µ–≥–æ: {nat_count + spam_count} —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
                    print("="*80)
                    print("üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:")
                    print("   ‚Ä¢ –û–ø—Ü–∏—è 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
                    print("="*80)
                    
                except ImportError as e:
                    print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –º–æ–¥—É–ª—å create_test_dataset: {e}")
                    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª create_test_dataset.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
                    import traceback
                    traceback.print_exc()
                
            elif choice == '12':
                # –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã
                print("\n" + "="*80)
                print("üëã –°–ü–ê–°–ò–ë–û –ó–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –î–ï–¢–ï–ö–¢–û–†–ê –°–ü–ê–ú–ê!")
                print("="*80)
                print("\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:")
                print("   ‚Ä¢ –°–∫—Ä—ã—Ç—ã–µ –º–∞—Ä–∫–æ–≤—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ (HMM)")
                print("   ‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º –ë–∞—É–º–∞-–í–µ–ª—à–∞ (–æ–±—É—á–µ–Ω–∏–µ EM-–∞–ª–≥–æ—Ä–∏—Ç–º)")
                print("   ‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º –í–∏—Ç–µ—Ä–±–∏ (–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π)")
                print("   ‚Ä¢ POS-—Ç–µ–≥–≥–∏–Ω–≥ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
                print("   ‚Ä¢ –ú–∞—Ä–∫–æ–≤—Å–∫–∏–µ —Ü–µ–ø–∏ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∞–º–∞)")
                
                print("\nüéì –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏:")
                print("   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP)")
                print("   ‚Ä¢ –ú–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
                print("   ‚Ä¢ –î–µ—Ç–µ–∫—Ü–∏–∏ —Å–ø–∞–º–∞")
                
                print("\nüí° –£–¥–∞—á–∏ –≤ –≤–∞—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö!")
                print("="*80 + "\n")
                break
                
            else:
                # –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä
                print("\n" + "="*80)
                print("‚ö†Ô∏è –ù–ï–í–ï–†–ù–´–ô –í–´–ë–û–†!")
                print("="*80)
                print("üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏: 1-12")
                print("   –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 12 –∏–∑ –º–µ–Ω—é –≤—ã—à–µ")
                print("="*80)
        
        except KeyboardInterrupt:
            print("\n\n" + "="*80)
            print("‚ö†Ô∏è –ü–†–û–ì–†–ê–ú–ú–ê –ü–†–ï–†–í–ê–ù–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú (Ctrl+C)")
            print("="*80)
            
            confirm = input("\n–í—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ö–æ—Ç–∏—Ç–µ –≤—ã–π—Ç–∏? (y/n): ").strip().lower()
            if confirm in ['y', 'yes', '–¥', '–¥–∞']:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            else:
                print("\n‚Ü©Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É...")
                continue
                
        except Exception as e:
            print("\n" + "="*80)
            print(f"‚ùå –ù–ï–û–ñ–ò–î–ê–ù–ù–ê–Ø –û–®–ò–ë–ö–ê")
            print("="*80)
            print(f"–û—à–∏–±–∫–∞: {e}")
            print("\nüìã –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:")
            import traceback
            traceback.print_exc()
            print("="*80)
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ —Å–æ–æ–±—â–∏—Ç–µ –æ–± –æ—à–∏–±–∫–µ")
        
        # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–µ–π
        input("\n‚Üµ –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
        print("\n" * 2)  # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\n" + "="*80)
        print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print("="*80)



if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
