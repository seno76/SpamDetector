"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from data_loader import DataLoader
from preprocessor import TextPreprocessor
from hmm_detector import SpamDetectorHMM
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime

class BatchAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def __init__(self, detector, preprocessor):
        self.detector = detector
        self.preprocessor = preprocessor
        self.results = []
    
    def analyze_single(self, text, true_label=None):
        """
        –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        
        Returns:
            dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        seq = self.preprocessor.text_to_sequence(text)
        result = self.detector.predict_proba(seq)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        tokens = self.preprocessor.tokenize(text)
        pos_tags = self.preprocessor.extract_pos_features(text)
        unk_idx = self.preprocessor.vocab.get('UNK', self.preprocessor.get_vocabulary_size() - 1)
        unk_ratio = float((seq == unk_idx).sum()) / len(seq) if len(seq) > 0 else 0.0
        
        # –í–∏—Ç–µ—Ä–±–∏ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        vit_nat = self.detector.decode_viterbi(seq, 'natural')
        vit_spam = self.detector.decode_viterbi(seq, 'spam')
        
        # –ü–æ—Å—Ç–µ—Ä–∏–æ—Ä—ã
        gamma_nat = self.detector.get_posteriors(seq, 'natural')
        gamma_spam = self.detector.get_posteriors(seq, 'spam')
        
        analysis = {
            'text_preview': text[:100],
            'text_length': len(text),
            'tokens_count': len(tokens),
            'sequence_length': len(seq),
            'unique_features': len(np.unique(seq)),
            'unk_ratio': unk_ratio,
            'prediction': result['prediction'],
            'prob_natural': result['prob_natural'],
            'prob_spam': result['prob_spam'],
            'log_prob_natural': result['log_prob_natural'],
            'log_prob_spam': result['log_prob_spam'],
            'log_diff': abs(result['log_prob_natural'] - result['log_prob_spam']),
            'viterbi_natural_logprob': vit_nat['log_probability'],
            'viterbi_spam_logprob': vit_spam['log_probability'],
            'viterbi_natural_states_used': vit_nat['n_states_used'],
            'viterbi_spam_states_used': vit_spam['n_states_used'],
            'avg_posterior_natural': gamma_nat.mean(axis=0).tolist() if gamma_nat.size > 0 else [],
            'avg_posterior_spam': gamma_spam.mean(axis=0).tolist() if gamma_spam.size > 0 else [],
            'true_label': true_label,
            'correct': (result['prediction'] == true_label) if true_label else None,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results.append(analysis)
        return analysis
    
    def analyze_batch(self, texts, true_labels=None, show_progress=True):
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            true_labels: —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            show_progress: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        if true_labels is None:
            true_labels = [None] * len(texts)
        
        print(f"\nüìä –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        for i, (text, label) in enumerate(zip(texts, true_labels)):
            if show_progress and (i + 1) % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(texts)}", end='\r')
            
            try:
                self.analyze_single(text, label)
            except Exception as e:
                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞ {i}: {e}")
        
        if show_progress:
            print(f"\n‚úì –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(self.results)} —Ç–µ–∫—Å—Ç–æ–≤")
    
    def get_statistics(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not self.results:
            print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return None
        
        df = pd.DataFrame(self.results)
        
        stats = {
            'total_texts': len(df),
            'predictions': {
                'natural': int((df['prediction'] == 'natural').sum()),
                'spam': int((df['prediction'] == 'spam').sum())
            },
            'avg_prob_natural': float(df['prob_natural'].mean()),
            'avg_prob_spam': float(df['prob_spam'].mean()),
            'avg_sequence_length': float(df['sequence_length'].mean()),
            'avg_unk_ratio': float(df['unk_ratio'].mean()),
            'avg_log_diff': float(df['log_diff'].mean())
        }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ - –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        if df['true_label'].notna().any():
            valid_df = df[df['true_label'].notna()]
            stats['accuracy'] = float((valid_df['prediction'] == valid_df['true_label']).mean())
            stats['correct_predictions'] = int((valid_df['prediction'] == valid_df['true_label']).sum())
            stats['incorrect_predictions'] = int((valid_df['prediction'] != valid_df['true_label']).sum())
        
        return stats
    
    def export_results(self, filepath='batch_results.json'):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {filepath}")
    
    def export_csv(self, filepath='batch_results.csv'):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV –¥–ª—è Excel"""
        df = pd.DataFrame(self.results)
        # –£–±–∏—Ä–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –ø–æ–ª—è –¥–ª—è CSV
        simple_df = df.drop(columns=['avg_posterior_natural', 'avg_posterior_spam'], errors='ignore')
        simple_df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {filepath}")
    
    def plot_statistics(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.results:
            print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return
        
        df = pd.DataFrame(self.results)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_counts = df['prediction'].value_counts()
        axes[0, 0].bar(pred_counts.index, pred_counts.values, color=['green', 'red'])
        axes[0, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
        axes[0, 0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        
        # 2. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ø–∞–º–∞
        axes[0, 1].hist(df['prob_spam'], bins=30, edgecolor='black', alpha=0.7, color='orange')
        axes[0, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ P(Spam|X)')
        axes[0, 1].set_xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
        axes[0, 1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        
        # 3. Scatter: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ vs –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ø–∞–º–∞
        axes[0, 2].scatter(df['sequence_length'], df['prob_spam'], alpha=0.5, c=df['prediction'].map({'natural': 'green', 'spam': 'red'}))
        axes[0, 2].set_title('–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ vs P(Spam)')
        axes[0, 2].set_xlabel('–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
        axes[0, 2].set_ylabel('P(Spam)')
        
        # 4. UNK ratio
        axes[1, 0].hist(df['unk_ratio'], bins=20, edgecolor='black', alpha=0.7, color='purple')
        axes[1, 0].set_title('–î–æ–ª—è UNK-—Ç–æ–∫–µ–Ω–æ–≤')
        axes[1, 0].set_xlabel('UNK Ratio')
        axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        
        # 5. Log-likelihood —Ä–∞–∑–Ω–∏—Ü–∞
        axes[1, 1].hist(df['log_diff'], bins=30, edgecolor='black', alpha=0.7, color='blue')
        axes[1, 1].set_title('–†–∞–∑–Ω–æ—Å—Ç—å –ª–æ–≥-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–π')
        axes[1, 1].set_xlabel('|LL_natural - LL_spam|')
        axes[1, 1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        
        # 6. Confusion matrix (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏)
        if df['true_label'].notna().any():
            valid_df = df[df['true_label'].notna()]
            cm = confusion_matrix(valid_df['true_label'], valid_df['prediction'], labels=['natural', 'spam'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2],
                       xticklabels=['Natural', 'Spam'], yticklabels=['Natural', 'Spam'])
            axes[1, 2].set_title('Confusion Matrix')
            axes[1, 2].set_ylabel('True')
            axes[1, 2].set_xlabel('Predicted')
        else:
            axes[1, 2].text(0.5, 0.5, '–ù–µ—Ç –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫', ha='center', va='center', fontsize=12)
            axes[1, 2].set_title('Confusion Matrix (N/A)')
        
        plt.tight_layout()
        plt.savefig('batch_statistics.png', dpi=300, bbox_inches='tight')
        print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: batch_statistics.png")
        plt.show()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    print("="*70)
    print("–ú–ê–°–°–û–í–´–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í –ù–ê –°–ü–ê–ú")
    print("="*70)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    detector = SpamDetectorHMM()
    detector.load('models/')
    
    preprocessor = TextPreprocessor(feature_type='pos', n_symbols=50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ)
    natural_texts, spam_texts = DataLoader.load_train_data()
    
    # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è inference)
    all_texts = natural_texts + spam_texts
    preprocessor.build_vocabulary(all_texts)
    
    # –°–æ–∑–¥–∞—ë–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = BatchAnalyzer(detector, preprocessor)
    
    # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
    texts = natural_texts + spam_texts
    labels = ['natural'] * len(natural_texts) + ['spam'] * len(spam_texts)
    
    analyzer.analyze_batch(texts, labels)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*70)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*70)
    stats = analyzer.get_statistics()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    analyzer.export_results('batch_results.json')
    analyzer.export_csv('batch_results.csv')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    analyzer.plot_statistics()
    
    # Classification report
    df = pd.DataFrame(analyzer.results)
    if df['true_label'].notna().any():
        valid_df = df[df['true_label'].notna()]
        print("\n" + "="*70)
        print("CLASSIFICATION REPORT")
        print("="*70)
        print(classification_report(valid_df['true_label'], valid_df['prediction'], 
                                   target_names=['Natural', 'Spam'], zero_division=0))
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìÅ –§–∞–π–ª—ã:")
    print(f"   ‚Ä¢ batch_results.json - –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    print(f"   ‚Ä¢ batch_results.csv - –¥–ª—è Excel")
    print(f"   ‚Ä¢ batch_statistics.png - –≥—Ä–∞—Ñ–∏–∫–∏")

if __name__ == "__main__":
    main()
